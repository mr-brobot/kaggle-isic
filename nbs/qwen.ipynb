{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce9104a",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e610a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2299034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"class_power\": 0.9,  # between 0 and 1 (inclusive), scales positive class weight (0 removes class weighting, 1 leaves class ratio unchanged)\n",
    "    \"focal_power\": 2,  # focusing parameter (gamma) in focal loss (default 2)\n",
    "    \"threshold\": 0.5,  # probability threshold for positive classification\n",
    "    \"seed\": 42,  # rng seed for reproducibility\n",
    "}\n",
    "\n",
    "epochs = params[\"epochs\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "lr = params[\"learning_rate\"]\n",
    "class_power = params[\"class_power\"]\n",
    "focal_power = params[\"focal_power\"]\n",
    "\n",
    "threshold = params[\"threshold\"]\n",
    "seed = params[\"seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695d33ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b589575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e936d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702f823",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fcf38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401059"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from isic.dataset import MetadataTextFormatter, MessagesFormatter\n",
    "\n",
    "ds = load_dataset(\"mrbrobot/isic-2024\", split=\"train\")\n",
    "ds = ds.select_columns([\"image\", \"age_approx\", \"sex\", \"anatom_site_general\", \"target\"])\n",
    "\n",
    "# format metadata as text\n",
    "ds = ds.with_format(\"arrow\")\n",
    "ds = ds.map(MetadataTextFormatter(), batched=True, desc=\"Formatting metadata\")\n",
    "\n",
    "# format metadata & image as prompt\n",
    "ds = ds.select_columns([\"image\", \"text\", \"target\"])\n",
    "ds = ds.with_format(None)\n",
    "ds = ds.with_transform(MessagesFormatter())\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a7fa1",
   "metadata": {},
   "source": [
    "# Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cf47b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3832f82a40464a2a988de7b60028e26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-4B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61830cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Classify provided example as benign or malignant.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=139x139>},\n",
       "   {'type': 'text',\n",
       "    'text': '| Field | Value |\\n|-------|-------|\\n| Age | 60 years |\\n| Sex | male |\\n| Lesion Site | lower extremity |'}]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = ds[0][\"messages\"]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e7dc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids -> <class 'torch.Tensor'>\n",
      "attention_mask -> <class 'torch.Tensor'>\n",
      "pixel_values -> <class 'torch.Tensor'>\n",
      "image_grid_thw -> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k} -> {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfff1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benign']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=4)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e55fd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3VLCausalLMOutputWithPast(loss=None, logits=tensor([[[ 3.6250,  2.7188,  4.9688,  ..., -2.2344, -2.2344, -2.2344],\n",
       "         [ 2.5312,  2.6406,  2.8594,  ...,  1.2969,  1.2969,  1.2969],\n",
       "         [ 7.7812,  8.0625,  7.8438,  ...,  2.4219,  2.4219,  2.4219],\n",
       "         ...,\n",
       "         [ 4.1875,  4.6562,  9.3750,  ...,  0.1406,  0.1406,  0.1406],\n",
       "         [ 1.8984,  6.0312,  4.2812,  ..., -3.0625, -3.0625, -3.0625],\n",
       "         [ 8.4375, 12.5000, 15.5000,  ..., -3.1250, -3.1250, -3.1250]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None, rope_deltas=tensor([[-56]], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs, max_new_tokens=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f866839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3VLForConditionalGeneration(\n",
      "  (model): Qwen3VLModel(\n",
      "    (visual): Qwen3VLVisionModel(\n",
      "      (patch_embed): Qwen3VLVisionPatchEmbed(\n",
      "        (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "      )\n",
      "      (pos_embed): Embedding(2304, 1024)\n",
      "      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-23): 24 x Qwen3VLVisionBlock(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Qwen3VLVisionAttention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (mlp): Qwen3VLVisionMLP(\n",
      "            (linear_fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (linear_fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (act_fn): GELUTanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merger): Qwen3VLVisionPatchMerger(\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        (act_fn): GELU(approximate='none')\n",
      "        (linear_fc2): Linear(in_features=4096, out_features=2560, bias=True)\n",
      "      )\n",
      "      (deepstack_merger_list): ModuleList(\n",
      "        (0-2): 3 x Qwen3VLVisionPatchMerger(\n",
      "          (norm): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
      "          (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (act_fn): GELU(approximate='none')\n",
      "          (linear_fc2): Linear(in_features=4096, out_features=2560, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (language_model): Qwen3VLTextModel(\n",
      "      (embed_tokens): Embedding(151936, 2560)\n",
      "      (layers): ModuleList(\n",
      "        (0-35): 36 x Qwen3VLTextDecoderLayer(\n",
      "          (self_attn): Qwen3VLTextAttention(\n",
      "            (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
      "            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Qwen3VLTextMLP(\n",
      "            (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "            (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "            (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (input_layernorm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Qwen3VLTextRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1752e6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4437815808"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db03115",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390509f",
   "metadata": {},
   "source": [
    "Class imbalance measurement & handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e49c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "Benign: 400,666 samples\n",
      "Malignant: 393 samples\n",
      "Imbalance ratio: 1019.5:1\n"
     ]
    }
   ],
   "source": [
    "class_counts = [400666, 393]  # [benign, malignant] from EDA\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Benign: {class_counts[0]:,} samples\")\n",
    "print(f\"Malignant: {class_counts[1]:,} samples\")\n",
    "print(f\"Imbalance ratio: {class_counts[0] / class_counts[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a00503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive weight: 1019.5\n",
      "Scaled positive class weight: 510.0\n"
     ]
    }
   ],
   "source": [
    "df = ds.to_pandas()\n",
    "neg_count = (df[\"target\"] == 0).sum()\n",
    "pos_count = (df[\"target\"] == 1).sum()\n",
    "pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"Positive weight: {pos_weight:.1f}\")\n",
    "print(f\"Scaled positive class weight: {pos_weight**class_power:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea10eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isic.loss import WeightedFocalLoss\n",
    "\n",
    "scaled_pos_weight = torch.tensor([pos_weight**class_power], device=device)\n",
    "criterion = WeightedFocalLoss(pos_weight=scaled_pos_weight, gamma=focal_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "jcy76jp1v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 320,847\n",
      "Val samples: 80,212\n"
     ]
    }
   ],
   "source": [
    "split = ds.train_test_split(test_size=0.2, seed=seed)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"Train samples: {len(train_ds):,}\")\n",
    "print(f\"Val samples: {len(val_ds):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd8d33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3VLConfig {\n",
       "  \"architectures\": [\n",
       "    \"Qwen3VLForConditionalGeneration\"\n",
       "  ],\n",
       "  \"image_token_id\": 151655,\n",
       "  \"model_type\": \"qwen3_vl\",\n",
       "  \"text_config\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bos_token_id\": 151643,\n",
       "    \"dtype\": \"bfloat16\",\n",
       "    \"eos_token_id\": 151645,\n",
       "    \"head_dim\": 128,\n",
       "    \"hidden_act\": \"silu\",\n",
       "    \"hidden_size\": 2560,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 9728,\n",
       "    \"max_position_embeddings\": 262144,\n",
       "    \"model_type\": \"qwen3_vl_text\",\n",
       "    \"num_attention_heads\": 32,\n",
       "    \"num_hidden_layers\": 36,\n",
       "    \"num_key_value_heads\": 8,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_scaling\": {\n",
       "      \"mrope_interleaved\": true,\n",
       "      \"mrope_section\": [\n",
       "        24,\n",
       "        20,\n",
       "        20\n",
       "      ],\n",
       "      \"rope_type\": \"default\"\n",
       "    },\n",
       "    \"rope_theta\": 5000000,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 151936\n",
       "  },\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"video_token_id\": 151656,\n",
       "  \"vision_config\": {\n",
       "    \"deepstack_visual_indexes\": [\n",
       "      5,\n",
       "      11,\n",
       "      17\n",
       "    ],\n",
       "    \"depth\": 24,\n",
       "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "    \"hidden_size\": 1024,\n",
       "    \"in_channels\": 3,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"model_type\": \"qwen3_vl\",\n",
       "    \"num_heads\": 16,\n",
       "    \"num_position_embeddings\": 2304,\n",
       "    \"out_hidden_size\": 2560,\n",
       "    \"patch_size\": 16,\n",
       "    \"spatial_merge_size\": 2,\n",
       "    \"temporal_patch_size\": 2\n",
       "  },\n",
       "  \"vision_end_token_id\": 151653,\n",
       "  \"vision_start_token_id\": 151652\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6fvfqzdsg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2,561\n",
      "Total parameters: 4,437,818,369\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Replace lm_head with binary classifier\n",
    "hidden_size = model.config.text_config.hidden_size\n",
    "model.lm_head = nn.Linear(hidden_size, 1).to(model.device)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the classification head\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Total parameters: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "npcvqtiw7p",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isic.dataset import VLMCollator\n",
    "\n",
    "collator = VLMCollator(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vxkizq0nap",
   "metadata": {},
   "outputs": [],
   "source": "from isic.loss import WeightedFocalLoss, VLMLoss\nfrom isic.metrics import BinaryMetricsComputer\n\ncriterion = WeightedFocalLoss(pos_weight=scaled_pos_weight, gamma=focal_power)\n\ncompute_loss = VLMLoss(criterion)\n\n# Initialize metrics computer for HuggingFace Trainer\nmetrics_computer = BinaryMetricsComputer(\n    threshold=threshold,\n    device=device,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jy6op2oy3a9",
   "metadata": {},
   "outputs": [],
   "source": "from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    # output_dir=\"./results/qwen-phase1\",\n    num_train_epochs=2,  # Phase 1: short training with frozen backbone\n    per_device_train_batch_size=batch_size,  # Adjust based on GPU memory\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",  # Use AUROC for model selection\n    bf16=True,  # Mixed precision for efficiency\n    remove_unused_columns=False,  # Important: keep all columns for collator\n    data_seed=seed,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collator,\n    compute_loss_func=compute_loss,\n    compute_metrics=metrics_computer,  # Pass metrics computer instance directly\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wlgk8dgudr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: huggingface\n",
      "* Trackio metrics will be synced to Hugging Face Dataset: mrbrobot/trackio-dataset\n",
      "* Found existing space: https://huggingface.co/spaces/mrbrobot/trackio\n",
      "* View dashboard by going to: https://mrbrobot-trackio.hf.space/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://mrbrobot-trackio.hf.space/\" width=\"100%\" height=\"1000px\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Created new run: mrbrobot-1761314816\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40106' max='40106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40106/40106 6:44:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.391800</td>\n",
       "      <td>8.850162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.494800</td>\n",
       "      <td>1.561652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12 (_init_client_background):\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/current/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/local/python/current/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/trackio/run.py\", line 111, in _init_client_background\n",
      "    self._batch_sender()\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/trackio/run.py\", line 82, in _batch_sender\n",
      "    self._client.predict(\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/gradio_client/client.py\", line 505, in predict\n",
      "    ).result()\n",
      "      ^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/gradio_client/client.py\", line 1610, in result\n",
      "    return super().result(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/python/current/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/gradio_client/client.py\", line 1214, in _inner\n",
      "    predictions = _predict(*data, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/gradio_client/client.py\", line 1316, in _predict\n",
      "    event_id = self.client.send_data(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/gradio_client/client.py\", line 320, in send_data\n",
      "    req = httpx.post(\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_api.py\", line 304, in post\n",
      "    return request(\n",
      "           ^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_api.py\", line 109, in request\n",
      "    return client.request(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/current/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/workspaces/isic/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Run finished. Uploading logs to Trackio (please wait...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40106, training_loss=6.260735238941329, metrics={'train_runtime': 24249.4262, 'train_samples_per_second': 26.462, 'train_steps_per_second': 1.654, 'total_flos': 1.8230358175625964e+18, 'train_loss': 6.260735238941329, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}